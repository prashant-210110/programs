{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ff7b1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"AIzaSyA-k1C-0Avb9YJ3sRo_uYSex9qZx38Og4c\" \n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bbbfa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prasa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatGoogleGenerativeAI(profile={'max_input_tokens': 1048576, 'max_output_tokens': 65536, 'image_inputs': True, 'audio_inputs': True, 'pdf_inputs': True, 'video_inputs': True, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'image_tool_message': True, 'tool_choice': True}, google_api_key=SecretStr('**********'), model='gemini-2.5-flash-lite', client=<google.genai.client.Client object at 0x000001B517A93770>, default_metadata=(), model_kwargs={})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyBpWCTAp_sqdwBofsSt1txt7XeS5ZLLdmw\"\n",
    "\n",
    "model = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5be6f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GoogleGenerativeAIEmbeddings(client=<google.genai.client.Client object at 0x000001B517A97610>, model='models/gemini-embedding-001', task_type=None, google_api_key=SecretStr('**********'), credentials=None, vertexai=None, project=None, location=None, base_url=None, additional_headers=None, client_args=None, request_options=None, output_dimensionality=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ac97c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_core.vectorstores.in_memory.InMemoryVectorStore at 0x1b517c2e270>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f16ee560",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 11561\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTransformers in Machine Learning - GeeksforGeeks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCoursesDSA / PlacementsGATE PrepML & Data ScienceDevelopmentCloud / DevOpsProgramming LanguagesAll CoursesTutorialsPythonJavaDSAML & Data ScienceInterview CornerProgramming LanguagesWeb DevelopmentGATECS SubjectsDevOpsSchool LearningSoftware and ToolsPracticePractice Coding ProblemsNation Skillup- Ending Soon!Problem of the DayStudy Abroad ChampionshipJobsApply Now!Post JobsJobs UpdatesApply for Campus Mantri\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNotifications\\n\\nMark all as read\\n\\n\\n\\n\\n\\nAll\\n\\n\\n\\n\\n\\n \\n\\nView All\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNotifications\\n\\n\\n\\nMark all as read\\n\\n\\n\\n\\n\\n\\nAll\\n\\n\\nUnread\\n\\n\\nRead\\n\\n\\n\\n\\n\\r\\n                        You\\'re all caught up!!\\r\\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPython for Machine LearningMachine Learning with RMachine Learning AlgorithmsEDAMath for Machine LearningMachine Learning Interview QuestionsML ProjectsDeep LearningNLPComputer visionData ScienceArtificial Intelligence \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign In\\n\\n\\n▲\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOpen In App\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTechnical Scripter ExploreMachine Learning BasicsIntroduction to Machine LearningTypes of Machine LearningWhat is Machine Learning Pipeline?Applications of Machine LearningPython for Machine LearningMachine Learning with Python TutorialNumPy Tutorial - Python LibraryPandas TutorialData Preprocessing in PythonEDA - Exploratory Data Analysis in PythonFeature EngineeringWhat is Feature Engineering?Introduction to Dimensionality ReductionFeature Selection Techniques in Machine LearningSupervised LearningSupervised Machine LearningLinear Regression in Machine learningLogistic Regression in Machine LearningDecision Tree in Machine LearningRandom Forest Algorithm in Machine LearningK-Nearest Neighbor(KNN) AlgorithmSupport Vector Machine (SVM) AlgorithmNaive Bayes ClassifiersUnsupervised LearningWhat is Unsupervised LearningK means Clustering – IntroductionHierarchical Clustering in Machine LearningDBSCAN Clustering in ML - Density based clusteringApriori AlgorithmFrequent Pattern Growth AlgorithmECLAT Algorithm - MLPrincipal Component Analysis (PCA)Model Evaluation and TuningEvaluation Metrics in Machine LearningRegularization in Machine LearningCross Validation in Machine LearningHyperparameter TuningUnderfitting and Overfitting in MLBias and Variance in Machine LearningAdvanced TechniquesReinforcement LearningSemi-Supervised Learning in MLSelf-Supervised Learning (SSL)Ensemble LearningMachine Learning PracticeMachine Learning Interview Questions and Answers100+ Machine Learning Projects with Source CodeML & Data Science40% Off \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTransformers in Machine Learning\\n\\n\\n\\nLast Updated : \\n10 Dec, 2025\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nComments\\n\\n\\n\\n\\n\\n\\n\\nImprove\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSuggest changes\\n\\n\\n\\n\\n\\n\\n16 Likes\\n\\n\\n\\nLike\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReport\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTransformer is a neural network architecture used for performing machine learning tasks particularly in natural language processing (NLP) and computer vision. In 2017 Vaswani et al. published a paper \" Attention is All You Need\" in which the transformers architecture was introduced. The article explores the architecture, workings and applications of transformers. Need For Transformers Model in Machine Learning Transformer architecture uses an attention mechanism to process an entire sentence at once instead of reading words one by one. This is useful because older models work step by step and it helps overcome the challenges seen in models like RNNs and LSTMs. Traditional models like RNNs (Recurrent Neural Networks) suffer from the vanishing gradient problem which leads to long-term memory loss. RNNs process text sequentially meaning they analyze words one at a time. For example: In the sentence: \"XYZ went to France in 2019 when there were no cases of COVID and there he met the president of that country\" the word \"that country\" refers to \"France\". However RNN would struggle to link \"that country\" to \"France\" since it processes each word in sequence leading to losing context over long sentences. This limitation prevents RNNs from understanding the full meaning of the sentence.While adding more memory cells in LSTMs (Long Short-Term Memory networks) helped address the vanishing gradient issue they still process words one by one. This sequential processing means LSTMs can\\'t analyze an entire sentence at once.For example: The word \"point\" has different meanings in these two sentences:\"The needle has a sharp point.\" (Point = Tip)\"It is not polite to point at people.\" (Point = Gesture)Traditional models struggle with this context dependence, whereas Transformer model through its self-attention mechanism processes the entire sentence in parallel addressing these issues and making it significantly more effective at understanding context.Core Concepts of TransformersArchitecture of Transformer1. Self Attention Mechanism The self attention mechanism allows transformers to determine which words in a sentence are most relevant to each other. This is done using a scaled dot-product attention approach:Each word in a sequence is mapped to three vectors:Query (Q)Key (K)Value (V)Attention scores are computed as: \\\\text{Attention}(Q, K, V) = \\\\text{softmax} \\\\left( \\\\frac{QK^T}{\\\\sqrt{d_k}} \\\\right) VThese scores determine how much attention each word should pay to others.2. Multi-Head AttentionInstead of one attention mechanism, transformers use multiple attention heads running in parallel. Each head captures different relationships or patterns in the data, enriching the model’s understanding.3. Positional Encoding Unlike RNNs, transformers lack an inherent understanding of word order since they process data in parallel. To solve this problem Positional Encodings are added to token embeddings providing information about the position of each token within a sequence.4. Position-wise Feed-Forward NetworksThe Feed-Forward Networks consist of two linear transformations with a ReLU activation. It is applied independently to each position in the sequence.Mathematically:\\\\text{FFN}(x) = \\\\max(0, xW_1 + b_1)W_2 + b_2This transformation helps refine the encoded representation at each position.5. EmbeddingsTransformers cannot work with raw words as they need numbers. So, each input token (word or subword) is converted into a vector, called an embedding.Both encoder input tokens and decoder input tokens are converted into embeddings.These embeddings are trainable, meaning the model learns the best numeric representation for each token.The same weight matrix is shared for Encoder embeddings, Decoder embeddings and the final linear layer before softmaxThe embeddings are scaled by model to keep values stable before adding positional encoding.Embeddings turn words into meaningful numeric vectors that the transformer can process.6. Encoder-Decoder Architecture The encoder-decoder structure is key to transformer models. The encoder processes the input sequence into a vector, while the decoder converts this vector back into a sequence. Each encoder and decoder layer includes self-attention and feed-forward layers.For example, a French sentence \"Je suis étudiant\" is translated into \"I am a student\" in English.Transformers apply attention in three different places:1. Encoder Self-AttentionQ, K, V all come from the encoder’s previous layer.Every word can attend to every other word in the input.This helps the encoder understand full context (long-range meaning).2. Decoder Self-Attention (Masked)Q, K, V all come from the decoder’s previous layer.Future tokens are masked (blocked), so each position only sees previous tokens.This keeps decoding auto-regressive i.e the model predicts one word at a time.3. Encoder–Decoder AttentionQueries come from the decoder.Keys and Values come from the encoder output.This lets the decoder look at important parts of the input sentence while generating output.Together, these three attention types allow the transformer to read the entire input at once and then generate outputs step-by-step with full context.7. Softmax Layer for Output PredictionAfter the decoder processes the sequence, it must predict the next token.The decoder output is passed through a linear layer (whose weights are shared with embeddings).Then the softmax function converts these scores into probabilities.The token with the highest probability becomes the predicted next word.Intuition with ExampleFor instance in the sentence \"The cat didn\\'t chase the mouse, because it was not hungry\" the word \\'it\\' refers to \\'cat\\'. The self-attention mechanism helps the model correctly associate \\'it\\' with \\'cat\\' ensuring an accurate understanding of sentence structure.Applications Some of the applications of transformers are: NLP Tasks: Transformers are used for machine translation, text summarization, named entity recognition and sentiment analysis.Speech Recognition: They process audio signals to convert speech into transcribed text.Computer Vision: Transformers are applied to image classification, object detection and image generation.Recommendation Systems: They provide personalized recommendations based on user preferences.Text and Music Generation: Transformers are used for generating text like articles and composing music. \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCreate Quiz\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTransformers in Machine Learning\\n\\n\\nVisit Course\\n\\n\\n\\n\\n\\n\\n\\nTransformers in Machine Learning\\n\\n\\n\\nTransformers Explained | Natural Language Processing (NLP)\\n\\n \\n\\n\\n\\n\\n\\r\\n        Comment\\r\\n    \\n\\n\\n\\n\\n\\nE\\n\\n\\n\\n\\n\\nerror_502\\n\\n\\n\\n\\n\\n Follow\\n\\n\\n\\n\\n\\n\\n16\\n\\n\\nImprove\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\nE\\n\\n\\n\\n\\n \\n\\nerror_502 \\n\\n\\n\\n\\n\\n Follow \\n\\n\\n\\n\\n\\n\\n\\n\\n16\\n\\n\\n\\nImprove\\n\\n\\n\\n\\n\\n\\nArticle Tags : \\n\\n\\nMachine Learning\\n\\n\\nAI-ML-DS\\n\\n\\nDeep-Learning\\n \\n\\n\\n\\n\\n\\n\\n\\n\\nLike\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCorporate & Communications Address:\\n\\r\\n                      A-143, 7th Floor, Sovereign Corporate Tower, Sector- 136, Noida, Uttar Pradesh (201305)                    \\n\\n\\n\\n\\n\\nRegistered Address:\\r\\n                        K 061, Tower K, Gulshan Vivante Apartment, Sector 137, Noida, Gautam Buddh Nagar, Uttar Pradesh, 201305                      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCompanyAbout UsLegalPrivacy PolicyCareersContact UsCorporate SolutionCampus Training ProgramExplorePOTDJob-A-ThonConnectBlogsNation Skill UpTutorialsProgramming LanguagesDSAWeb TechnologyAI, ML & Data ScienceDevOpsCS Core SubjectsGATESchool SubjectsSoftware and ToolsCoursesML and Data ScienceDSA and PlacementsWeb DevelopmentData ScienceProgramming LanguagesDevOps & CloudGATETrending TechnologiesOffline CentersNoidaBengaluruPuneHyderabadKolkataPreparation CornerInterview CornerAptitudePuzzlesGfG 160System Design \\n\\n\\n\\n\\n\\n\\n@GeeksforGeeks, Sanchhaya Education Private Limited, All rights reserved\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nImprovement\\n\\n\\n\\n\\n\\n\\n\\nSuggest changes\\n\\n\\n\\n\\n\\nSuggest Changes\\nHelp us improve. Share your suggestions to enhance the article. Contribute your expertise and make a difference in the GeeksforGeeks portal.\\n\\n\\n\\n\\n\\n\\n\\nCreate Improvement\\nEnhance the article with your expertise. Contribute to the GeeksforGeeks community and help create better learning resources for all.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSuggest Changes\\n\\n\\n\\n\\n\\n\\nmin 4 words, max Words Limit:1000\\n\\n\\n\\n\\nThank You!\\nYour suggestions are valuable to us.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat kind of Experience do you want to share?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nInterview Experiences\\n\\n\\n\\n\\n\\n\\n\\nAdmission Experiences\\n\\n\\n\\n\\n\\n\\n\\nCareer Journeys\\n\\n\\n\\n\\n\\n\\n\\nWork Experiences\\n\\n\\n\\n\\n\\n\\n\\nCampus Experiences\\n\\n\\n\\n\\n\\n\\n\\nCompetitive Exam Experiences\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing ot loading the data sources\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader,TextLoader\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/\",)\n",
    ")\n",
    "#loader=TextLoader(\"c:\\DATA SCIENCE\\Machine_Learning\\Datasets\\Salary_dataset.csv\")\n",
    "# Choose the loader based on your file type: \n",
    "# loader = CSVLoader(sample_path) # For CSV \n",
    "# loader = TextLoader(\"path/to/file.txt\") \n",
    "# loader = PyPDFLoader(\"path/to/file.pdf\") \n",
    "# loader = UnstructuredWordDocumentLoader(\"path/to/file.docx\")\n",
    "docs = loader.load()\n",
    "\n",
    "assert len(docs) == 1\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81dc03ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 16 sub-documents.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US', 'start_index': 48}, page_content=\"Transformers in Machine Learning - GeeksforGeeks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCoursesDSA / PlacementsGATE PrepML & Data ScienceDevelopmentCloud / DevOpsProgramming LanguagesAll CoursesTutorialsPythonJavaDSAML & Data ScienceInterview CornerProgramming LanguagesWeb DevelopmentGATECS SubjectsDevOpsSchool LearningSoftware and ToolsPracticePractice Coding ProblemsNation Skillup- Ending Soon!Problem of the DayStudy Abroad ChampionshipJobsApply Now!Post JobsJobs UpdatesApply for Campus Mantri\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNotifications\\n\\nMark all as read\\n\\n\\n\\n\\n\\nAll\\n\\n\\n\\n\\n\\n \\n\\nView All\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNotifications\\n\\n\\n\\nMark all as read\\n\\n\\n\\n\\n\\n\\nAll\\n\\n\\nUnread\\n\\n\\nRead\\n\\n\\n\\n\\n\\r\\n                        You're all caught up!!\"),\n",
       " Document(metadata={'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US', 'start_index': 651}, page_content=\"All\\n\\n\\n\\n\\n\\n \\n\\nView All\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNotifications\\n\\n\\n\\nMark all as read\\n\\n\\n\\n\\n\\n\\nAll\\n\\n\\nUnread\\n\\n\\nRead\\n\\n\\n\\n\\n\\r\\n                        You're all caught up!!\\r\\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPython for Machine LearningMachine Learning with RMachine Learning AlgorithmsEDAMath for Machine LearningMachine Learning Interview QuestionsML ProjectsDeep LearningNLPComputer visionData ScienceArtificial Intelligence \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign In\\n\\n\\n▲\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOpen In App\"),\n",
       " Document(metadata={'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US', 'start_index': 1129}, page_content='Technical Scripter ExploreMachine Learning BasicsIntroduction to Machine LearningTypes of Machine LearningWhat is Machine Learning Pipeline?Applications of Machine LearningPython for Machine LearningMachine Learning with Python TutorialNumPy Tutorial - Python LibraryPandas TutorialData Preprocessing in PythonEDA - Exploratory Data Analysis in PythonFeature EngineeringWhat is Feature Engineering?Introduction to Dimensionality ReductionFeature Selection Techniques in Machine LearningSupervised LearningSupervised Machine LearningLinear Regression in Machine learningLogistic Regression in Machine LearningDecision Tree in Machine LearningRandom Forest Algorithm in Machine LearningK-Nearest Neighbor(KNN) AlgorithmSupport Vector Machine (SVM) AlgorithmNaive Bayes ClassifiersUnsupervised LearningWhat is Unsupervised LearningK means Clustering – IntroductionHierarchical Clustering in Machine LearningDBSCAN Clustering in ML - Density based clusteringApriori AlgorithmFrequent Pattern Growth'),\n",
       " Document(metadata={'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US', 'start_index': 1933}, page_content='is Unsupervised LearningK means Clustering – IntroductionHierarchical Clustering in Machine LearningDBSCAN Clustering in ML - Density based clusteringApriori AlgorithmFrequent Pattern Growth AlgorithmECLAT Algorithm - MLPrincipal Component Analysis (PCA)Model Evaluation and TuningEvaluation Metrics in Machine LearningRegularization in Machine LearningCross Validation in Machine LearningHyperparameter TuningUnderfitting and Overfitting in MLBias and Variance in Machine LearningAdvanced TechniquesReinforcement LearningSemi-Supervised Learning in MLSelf-Supervised Learning (SSL)Ensemble LearningMachine Learning PracticeMachine Learning Interview Questions and Answers100+ Machine Learning Projects with Source CodeML & Data Science40% Off'),\n",
       " Document(metadata={'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US', 'start_index': 2694}, page_content='Transformers in Machine Learning\\n\\n\\n\\nLast Updated : \\n10 Dec, 2025\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nComments\\n\\n\\n\\n\\n\\n\\n\\nImprove\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSuggest changes\\n\\n\\n\\n\\n\\n\\n16 Likes\\n\\n\\n\\nLike\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReport'),\n",
       " Document(metadata={'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US', 'start_index': 2888}, page_content='Transformer is a neural network architecture used for performing machine learning tasks particularly in natural language processing (NLP) and computer vision. In 2017 Vaswani et al. published a paper \" Attention is All You Need\" in which the transformers architecture was introduced. The article explores the architecture, workings and applications of transformers. Need For Transformers Model in Machine Learning Transformer architecture uses an attention mechanism to process an entire sentence at once instead of reading words one by one. This is useful because older models work step by step and it helps overcome the challenges seen in models like RNNs and LSTMs. Traditional models like RNNs (Recurrent Neural Networks) suffer from the vanishing gradient problem which leads to long-term memory loss. RNNs process text sequentially meaning they analyze words one at a time. For example: In the sentence: \"XYZ went to France in 2019 when there were no cases of COVID and there he met the'),\n",
       " Document(metadata={'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US', 'start_index': 3682}, page_content='memory loss. RNNs process text sequentially meaning they analyze words one at a time. For example: In the sentence: \"XYZ went to France in 2019 when there were no cases of COVID and there he met the president of that country\" the word \"that country\" refers to \"France\". However RNN would struggle to link \"that country\" to \"France\" since it processes each word in sequence leading to losing context over long sentences. This limitation prevents RNNs from understanding the full meaning of the sentence.While adding more memory cells in LSTMs (Long Short-Term Memory networks) helped address the vanishing gradient issue they still process words one by one. This sequential processing means LSTMs can\\'t analyze an entire sentence at once.For example: The word \"point\" has different meanings in these two sentences:\"The needle has a sharp point.\" (Point = Tip)\"It is not polite to point at people.\" (Point = Gesture)Traditional models struggle with this context dependence, whereas Transformer model'),\n",
       " Document(metadata={'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US', 'start_index': 4481}, page_content='two sentences:\"The needle has a sharp point.\" (Point = Tip)\"It is not polite to point at people.\" (Point = Gesture)Traditional models struggle with this context dependence, whereas Transformer model through its self-attention mechanism processes the entire sentence in parallel addressing these issues and making it significantly more effective at understanding context.Core Concepts of TransformersArchitecture of Transformer1. Self Attention Mechanism The self attention mechanism allows transformers to determine which words in a sentence are most relevant to each other. This is done using a scaled dot-product attention approach:Each word in a sequence is mapped to three vectors:Query (Q)Key (K)Value (V)Attention scores are computed as: \\\\text{Attention}(Q, K, V) = \\\\text{softmax} \\\\left( \\\\frac{QK^T}{\\\\sqrt{d_k}} \\\\right) VThese scores determine how much attention each word should pay to others.2. Multi-Head AttentionInstead of one attention mechanism, transformers use multiple attention'),\n",
       " Document(metadata={'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US', 'start_index': 5299}, page_content='\\\\right) VThese scores determine how much attention each word should pay to others.2. Multi-Head AttentionInstead of one attention mechanism, transformers use multiple attention heads running in parallel. Each head captures different relationships or patterns in the data, enriching the model’s understanding.3. Positional Encoding Unlike RNNs, transformers lack an inherent understanding of word order since they process data in parallel. To solve this problem Positional Encodings are added to token embeddings providing information about the position of each token within a sequence.4. Position-wise Feed-Forward NetworksThe Feed-Forward Networks consist of two linear transformations with a ReLU activation. It is applied independently to each position in the sequence.Mathematically:\\\\text{FFN}(x) = \\\\max(0, xW_1 + b_1)W_2 + b_2This transformation helps refine the encoded representation at each position.5. EmbeddingsTransformers cannot work with raw words as they need numbers. So, each input'),\n",
       " Document(metadata={'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US', 'start_index': 6100}, page_content='= \\\\max(0, xW_1 + b_1)W_2 + b_2This transformation helps refine the encoded representation at each position.5. EmbeddingsTransformers cannot work with raw words as they need numbers. So, each input token (word or subword) is converted into a vector, called an embedding.Both encoder input tokens and decoder input tokens are converted into embeddings.These embeddings are trainable, meaning the model learns the best numeric representation for each token.The same weight matrix is shared for Encoder embeddings, Decoder embeddings and the final linear layer before softmaxThe embeddings are scaled by model to keep values stable before adding positional encoding.Embeddings turn words into meaningful numeric vectors that the transformer can process.6. Encoder-Decoder Architecture The encoder-decoder structure is key to transformer models. The encoder processes the input sequence into a vector, while the decoder converts this vector back into a sequence. Each encoder and decoder layer includes'),\n",
       " Document(metadata={'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US', 'start_index': 6901}, page_content='structure is key to transformer models. The encoder processes the input sequence into a vector, while the decoder converts this vector back into a sequence. Each encoder and decoder layer includes self-attention and feed-forward layers.For example, a French sentence \"Je suis étudiant\" is translated into \"I am a student\" in English.Transformers apply attention in three different places:1. Encoder Self-AttentionQ, K, V all come from the encoder’s previous layer.Every word can attend to every other word in the input.This helps the encoder understand full context (long-range meaning).2. Decoder Self-Attention (Masked)Q, K, V all come from the decoder’s previous layer.Future tokens are masked (blocked), so each position only sees previous tokens.This keeps decoding auto-regressive i.e the model predicts one word at a time.3. Encoder–Decoder AttentionQueries come from the decoder.Keys and Values come from the encoder output.This lets the decoder look at important parts of the input sentence'),\n",
       " Document(metadata={'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US', 'start_index': 7702}, page_content='predicts one word at a time.3. Encoder–Decoder AttentionQueries come from the decoder.Keys and Values come from the encoder output.This lets the decoder look at important parts of the input sentence while generating output.Together, these three attention types allow the transformer to read the entire input at once and then generate outputs step-by-step with full context.7. Softmax Layer for Output PredictionAfter the decoder processes the sequence, it must predict the next token.The decoder output is passed through a linear layer (whose weights are shared with embeddings).Then the softmax function converts these scores into probabilities.The token with the highest probability becomes the predicted next word.Intuition with ExampleFor instance in the sentence \"The cat didn\\'t chase the mouse, because it was not hungry\" the word \\'it\\' refers to \\'cat\\'. The self-attention mechanism helps the model correctly associate \\'it\\' with \\'cat\\' ensuring an accurate understanding of sentence'),\n",
       " Document(metadata={'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US', 'start_index': 8492}, page_content='the mouse, because it was not hungry\" the word \\'it\\' refers to \\'cat\\'. The self-attention mechanism helps the model correctly associate \\'it\\' with \\'cat\\' ensuring an accurate understanding of sentence structure.Applications Some of the applications of transformers are: NLP Tasks: Transformers are used for machine translation, text summarization, named entity recognition and sentiment analysis.Speech Recognition: They process audio signals to convert speech into transcribed text.Computer Vision: Transformers are applied to image classification, object detection and image generation.Recommendation Systems: They provide personalized recommendations based on user preferences.Text and Music Generation: Transformers are used for generating text like articles and composing music.'),\n",
       " Document(metadata={'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US', 'start_index': 9283}, page_content='Create Quiz\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTransformers in Machine Learning\\n\\n\\nVisit Course\\n\\n\\n\\n\\n\\n\\n\\nTransformers in Machine Learning\\n\\n\\n\\nTransformers Explained | Natural Language Processing (NLP)\\n\\n \\n\\n\\n\\n\\n\\r\\n        Comment\\r\\n    \\n\\n\\n\\n\\n\\nE\\n\\n\\n\\n\\n\\nerror_502\\n\\n\\n\\n\\n\\n Follow\\n\\n\\n\\n\\n\\n\\n16\\n\\n\\nImprove\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\nE\\n\\n\\n\\n\\n \\n\\nerror_502 \\n\\n\\n\\n\\n\\n Follow \\n\\n\\n\\n\\n\\n\\n\\n\\n16\\n\\n\\n\\nImprove\\n\\n\\n\\n\\n\\n\\nArticle Tags : \\n\\n\\nMachine Learning\\n\\n\\nAI-ML-DS\\n\\n\\nDeep-Learning\\n \\n\\n\\n\\n\\n\\n\\n\\n\\nLike\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCorporate & Communications Address:\\n\\r\\n                      A-143, 7th Floor, Sovereign Corporate Tower, Sector- 136, Noida, Uttar Pradesh (201305)                    \\n\\n\\n\\n\\n\\nRegistered Address:\\r\\n                        K 061, Tower K, Gulshan Vivante Apartment, Sector 137, Noida, Gautam Buddh Nagar, Uttar Pradesh, 201305'),\n",
       " Document(metadata={'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US', 'start_index': 9999}, page_content='Registered Address:\\r\\n                        K 061, Tower K, Gulshan Vivante Apartment, Sector 137, Noida, Gautam Buddh Nagar, Uttar Pradesh, 201305                      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCompanyAbout UsLegalPrivacy PolicyCareersContact UsCorporate SolutionCampus Training ProgramExplorePOTDJob-A-ThonConnectBlogsNation Skill UpTutorialsProgramming LanguagesDSAWeb TechnologyAI, ML & Data ScienceDevOpsCS Core SubjectsGATESchool SubjectsSoftware and ToolsCoursesML and Data ScienceDSA and PlacementsWeb DevelopmentData ScienceProgramming LanguagesDevOps & CloudGATETrending TechnologiesOffline CentersNoidaBengaluruPuneHyderabadKolkataPreparation CornerInterview CornerAptitudePuzzlesGfG 160System Design \\n\\n\\n\\n\\n\\n\\n@GeeksforGeeks, Sanchhaya Education Private Limited, All rights reserved\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nImprovement\\n\\n\\n\\n\\n\\n\\n\\nSuggest changes'),\n",
       " Document(metadata={'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US', 'start_index': 10723}, page_content='@GeeksforGeeks, Sanchhaya Education Private Limited, All rights reserved\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nImprovement\\n\\n\\n\\n\\n\\n\\n\\nSuggest changes\\n\\n\\n\\n\\n\\nSuggest Changes\\nHelp us improve. Share your suggestions to enhance the article. Contribute your expertise and make a difference in the GeeksforGeeks portal.\\n\\n\\n\\n\\n\\n\\n\\nCreate Improvement\\nEnhance the article with your expertise. Contribute to the GeeksforGeeks community and help create better learning resources for all.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSuggest Changes\\n\\n\\n\\n\\n\\n\\nmin 4 words, max Words Limit:1000\\n\\n\\n\\n\\nThank You!\\nYour suggestions are valuable to us.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat kind of Experience do you want to share?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nInterview Experiences\\n\\n\\n\\n\\n\\n\\n\\nAdmission Experiences\\n\\n\\n\\n\\n\\n\\n\\nCareer Journeys\\n\\n\\n\\n\\n\\n\\n\\nWork Experiences\\n\\n\\n\\n\\n\\n\\n\\nCampus Experiences\\n\\n\\n\\n\\n\\n\\n\\nCompetitive Exam Experiences')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#splliting into chuncks\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")\n",
    "all_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d10086e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "GoogleGenerativeAIError",
     "evalue": "Error embedding content (INVALID_ARGUMENT): 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'API Key not found. Please pass a valid API key.', 'status': 'INVALID_ARGUMENT', 'details': [{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'API_KEY_INVALID', 'domain': 'googleapis.com', 'metadata': {'service': 'generativelanguage.googleapis.com'}}, {'@type': 'type.googleapis.com/google.rpc.LocalizedMessage', 'locale': 'en-US', 'message': 'API Key not found. Please pass a valid API key.'}]}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prasa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:432\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, batch_size, task_type, titles, output_dimensionality)\u001b[39m\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prasa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\genai\\models.py:4167\u001b[39m, in \u001b[36mModels.embed_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   4165\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m4167\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4168\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   4169\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4171\u001b[39m response_dict = {} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response.body \u001b[38;5;28;01melse\u001b[39;00m json.loads(response.body)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prasa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\genai\\_api_client.py:1388\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m   1385\u001b[39m http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m   1386\u001b[39m     http_method, path, request_dict, http_options\n\u001b[32m   1387\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1388\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1389\u001b[39m response_body = (\n\u001b[32m   1390\u001b[39m     response.response_stream[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m response.response_stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1391\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prasa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\genai\\_api_client.py:1224\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, http_options, stream)\u001b[39m\n\u001b[32m   1222\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retry(\u001b[38;5;28mself\u001b[39m._request_once, http_request, stream)  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1224\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_once\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prasa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tenacity\\__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prasa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tenacity\\__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prasa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tenacity\\__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prasa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tenacity\\__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prasa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prasa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m     \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prasa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tenacity\\__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prasa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\genai\\_api_client.py:1201\u001b[39m, in \u001b[36mBaseApiClient._request_once\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m   1194\u001b[39m response = \u001b[38;5;28mself\u001b[39m._httpx_client.request(\n\u001b[32m   1195\u001b[39m     method=http_request.method,\n\u001b[32m   1196\u001b[39m     url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1199\u001b[39m     timeout=http_request.timeout,\n\u001b[32m   1200\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1201\u001b[39m \u001b[43merrors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPIError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m   1203\u001b[39m     response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m   1204\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prasa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\genai\\errors.py:121\u001b[39m, in \u001b[36mAPIError.raise_for_response\u001b[39m\u001b[34m(cls, response)\u001b[39m\n\u001b[32m    119\u001b[39m   response_json = response.body_segments[\u001b[32m0\u001b[39m].get(\u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m, {})\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prasa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\genai\\errors.py:146\u001b[39m, in \u001b[36mAPIError.raise_error\u001b[39m\u001b[34m(cls, status_code, response_json, response)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m400\u001b[39m <= status_code < \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m500\u001b[39m <= status_code < \u001b[32m600\u001b[39m:\n",
      "\u001b[31mClientError\u001b[39m: 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'API Key not found. Please pass a valid API key.', 'status': 'INVALID_ARGUMENT', 'details': [{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'API_KEY_INVALID', 'domain': 'googleapis.com', 'metadata': {'service': 'generativelanguage.googleapis.com'}}, {'@type': 'type.googleapis.com/google.rpc.LocalizedMessage', 'locale': 'en-US', 'message': 'API Key not found. Please pass a valid API key.'}]}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mGoogleGenerativeAIError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#storing the documents\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m document_ids = \u001b[43mvector_store\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_splits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(document_ids[:\u001b[32m3\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prasa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\vectorstores\\in_memory.py:194\u001b[39m, in \u001b[36mInMemoryVectorStore.add_documents\u001b[39m\u001b[34m(self, documents, ids, **kwargs)\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34madd_documents\u001b[39m(\n\u001b[32m    188\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    191\u001b[39m     **kwargs: Any,\n\u001b[32m    192\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    193\u001b[39m     texts = [doc.page_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     vectors = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ids) != \u001b[38;5;28mlen\u001b[39m(texts):\n\u001b[32m    197\u001b[39m         msg = (\n\u001b[32m    198\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mids must be the same length as texts. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    199\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ids and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(texts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m texts.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    200\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prasa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:439\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, batch_size, task_type, titles, output_dimensionality)\u001b[39m\n\u001b[32m    437\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    438\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError embedding content (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me.status\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m GoogleGenerativeAIError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    440\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    441\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError embedding content: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mGoogleGenerativeAIError\u001b[39m: Error embedding content (INVALID_ARGUMENT): 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'API Key not found. Please pass a valid API key.', 'status': 'INVALID_ARGUMENT', 'details': [{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'API_KEY_INVALID', 'domain': 'googleapis.com', 'metadata': {'service': 'generativelanguage.googleapis.com'}}, {'@type': 'type.googleapis.com/google.rpc.LocalizedMessage', 'locale': 'en-US', 'message': 'API Key not found. Please pass a valid API key.'}]}}"
     ]
    }
   ],
   "source": [
    "#storing the documents\n",
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839a6d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88733a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x000001C0C5F16D50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "\n",
    "tools = [retrieve_context]\n",
    "# If desired, specify custom instructions\n",
    "prompt = (\n",
    "    \"You have access to a tool that retrieves context from a blog post. \"\n",
    "    \"Use the tool to help answer user queries.\"\n",
    ")\n",
    "agent = create_agent(model, tools, system_prompt=prompt)\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90323340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what is the overall view of the website\n",
      "what is transformer \n",
      "what is the use of transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_context (3de91b90-ff29-4348-8096-2a5ef084c664)\n",
      " Call ID: 3de91b90-ff29-4348-8096-2a5ef084c664\n",
      "  Args:\n",
      "    query: overall view of the website\n",
      "  retrieve_context (b5fad0ec-ad2f-4c7e-9618-14a5bceff1c1)\n",
      " Call ID: b5fad0ec-ad2f-4c7e-9618-14a5bceff1c1\n",
      "  Args:\n",
      "    query: what is transformer\n",
      "  retrieve_context (078d6c40-4a1e-461c-b312-d7f8cabd5fde)\n",
      " Call ID: 078d6c40-4a1e-461c-b312-d7f8cabd5fde\n",
      "  Args:\n",
      "    query: what is the use of transformer\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Source: {'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US', 'start_index': 2926}\n",
      "Content: Transformer is a neural network architecture used for performing machine learning tasks particularly in natural language processing (NLP) and computer vision. In 2017 Vaswani et al. published a paper \" Attention is All You Need\" in which the transformers architecture was introduced. The article explores the architecture, workings and applications of transformers. Need For Transformers Model in Machine Learning Transformer architecture uses an attention mechanism to process an entire sentence at once instead of reading words one by one. This is useful because older models work step by step and it helps overcome the challenges seen in models like RNNs and LSTMs. Traditional models like RNNs (Recurrent Neural Networks) suffer from the vanishing gradient problem which leads to long-term memory loss. RNNs process text sequentially meaning they analyze words one at a time. For example: In the sentence: \"XYZ went to France in 2019 when there were no cases of COVID and there he met the\n",
      "\n",
      "Source: {'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US', 'start_index': 8530}\n",
      "Content: the mouse, because it was not hungry\" the word 'it' refers to 'cat'. The self-attention mechanism helps the model correctly associate 'it' with 'cat' ensuring an accurate understanding of sentence structure.Applications Some of the applications of transformers are: NLP Tasks: Transformers are used for machine translation, text summarization, named entity recognition and sentiment analysis.Speech Recognition: They process audio signals to convert speech into transcribed text.Computer Vision: Transformers are applied to image classification, object detection and image generation.Recommendation Systems: They provide personalized recommendations based on user preferences.Text and Music Generation: Transformers are used for generating text like articles and composing music.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The website, GeeksforGeeks, is a comprehensive educational platform covering various domains such as computer science, programming, school education, upskilling, and more.\n",
      "\n",
      "A Transformer is a neural network architecture introduced in 2017, primarily used for machine learning tasks in Natural Language Processing (NLP) and computer vision. It utilizes an attention mechanism to process an entire sequence (like a sentence) at once, overcoming limitations of older sequential models like RNNs and LSTMs that suffer from issues like the vanishing gradient problem.\n",
      "\n",
      "The Transformer's attention mechanism allows it to understand the context of words in a sentence more effectively, for example, correctly associating pronouns with their referents. Its applications are diverse and include machine translation, text summarization, speech recognition, image classification, and even text and music generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "    \"what is the overall view of the website\\n\"\n",
    "    \"what is transformer \\n\"\n",
    "    \"what is the use of transformer\"\n",
    ")\n",
    "\n",
    "for event in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
