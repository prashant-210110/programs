{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25b69587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: langchain-text-splitters in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: bs4 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.2.1 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (1.2.3)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (1.0.5)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (2.12.5)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.4.59)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\prasa\\appdata\\roaming\\python\\python313\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.12.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain) (3.0.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
      "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.3.0)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.1)\n",
      "Requirement already satisfied: httpx>=0.25.2 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (0.25.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (4.12.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community) (1.0.0)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community) (2.0.45)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community) (3.13.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community) (2.12.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=2.1.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community) (2.3.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (2.3.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from bs4) (4.14.3)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from beautifulsoup4->bs4) (2.8)\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain langchain-text-splitters langchain-community bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ff7b1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"AIzaSyA-k1C-0Avb9YJ3sRo_uYSex9qZx38Og4c\" \n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "623cf830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain[google-genai] in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.2.1 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain[google-genai]) (1.2.3)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain[google-genai]) (1.0.5)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain[google-genai]) (2.12.5)\n",
      "Requirement already satisfied: langchain-google-genai in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain[google-genai]) (4.1.1)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain[google-genai]) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain[google-genai]) (0.4.59)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\prasa\\appdata\\roaming\\python\\python313\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain[google-genai]) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain[google-genai]) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain[google-genai]) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain[google-genai]) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain[google-genai]) (0.12.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain[google-genai]) (3.0.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (1.0.5)\n",
      "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (0.3.0)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (3.6.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (1.12.1)\n",
      "Requirement already satisfied: httpx>=0.25.2 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain[google-genai]) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain[google-genai]) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain[google-genai]) (0.25.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (4.12.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain[google-genai]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain[google-genai]) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain[google-genai]) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain[google-genai]) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain[google-genai]) (2.3.0)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-google-genai->langchain[google-genai]) (1.2.0)\n",
      "Requirement already satisfied: google-genai<2.0.0,>=1.53.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-google-genai->langchain[google-genai]) (1.55.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai->langchain[google-genai]) (2.45.0)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai->langchain[google-genai]) (15.0.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai->langchain[google-genai]) (1.9.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai->langchain[google-genai]) (1.3.1)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai->langchain[google-genai]) (6.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai->langchain[google-genai]) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai->langchain[google-genai]) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai->langchain[google-genai]) (0.6.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install -U \"langchain[google-genai]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bbbfa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prasa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatGoogleGenerativeAI(profile={'max_input_tokens': 1048576, 'max_output_tokens': 65536, 'image_inputs': True, 'audio_inputs': True, 'pdf_inputs': True, 'video_inputs': True, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'image_tool_message': True, 'tool_choice': True}, google_api_key=SecretStr('**********'), model='gemini-2.5-flash-lite', client=<google.genai.client.Client object at 0x000001C0A478BB60>, default_metadata=(), model_kwargs={})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyBpWCTAp_sqdwBofsSt1txt7XeS5ZLLdmw\"\n",
    "\n",
    "model = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "359abfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qU langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5be6f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GoogleGenerativeAIEmbeddings(client=<google.genai.client.Client object at 0x000001C0A4767750>, model='models/gemini-embedding-001', task_type=None, google_api_key=SecretStr('**********'), credentials=None, vertexai=None, project=None, location=None, base_url=None, additional_headers=None, client_args=None, request_options=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bc5d72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-core in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.2.3)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core) (0.4.59)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\prasa\\appdata\\roaming\\python\\python313\\site-packages (from langchain-core) (25.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core) (2.12.5)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core) (0.12.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (4.12.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prasa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install -U \"langchain-core\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ac97c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_core.vectorstores.in_memory.InMemoryVectorStore at 0x1c0a4922660>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f16ee560",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 11597\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTransformers in Machine Learning - GeeksforGeeks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCoursesDSA / PlacementsGATE PrepML & Data ScienceDevelopmentCloud / DevOpsProgramming LanguagesAll CoursesTutorialsPythonJavaDSAML & Data ScienceInterview CornerProgramming LanguagesWeb DevelopmentGATECS SubjectsDevOpsSchool LearningSoftware and ToolsPracticePractice Coding ProblemsNation Skillup- Ending Soon!Problem of the DayAI Agent Workshop (with Certification)Study Abroad ChampionshipJobsApply Now!Post JobsJobs UpdatesApply for Campus Mantri\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNotifications\\n\\nMark all as read\\n\\n\\n\\n\\n\\nAll\\n\\n\\n\\n\\n\\n \\n\\nView All\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNotifications\\n\\n\\n\\nMark all as read\\n\\n\\n\\n\\n\\n\\nAll\\n\\n\\nUnread\\n\\n\\nRead\\n\\n\\n\\n\\n\\r\\n                        You\\'re all caught up!!\\r\\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPython for Machine LearningMachine Learning with RMachine Learning AlgorithmsEDAMath for Machine LearningMachine Learning Interview QuestionsML ProjectsDeep LearningNLPComputer visionData ScienceArtificial Intelligence \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign In\\n\\n\\n▲\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOpen In App\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTechnical Scripter ExploreMachine Learning BasicsIntroduction to Machine LearningTypes of Machine LearningWhat is Machine Learning Pipeline?Applications of Machine LearningPython for Machine LearningMachine Learning with Python TutorialNumPy Tutorial - Python LibraryPandas TutorialData Preprocessing in PythonEDA - Exploratory Data Analysis in PythonFeature EngineeringWhat is Feature Engineering?Introduction to Dimensionality ReductionFeature Selection Techniques in Machine LearningSupervised LearningSupervised Machine LearningLinear Regression in Machine learningLogistic Regression in Machine LearningDecision Tree in Machine LearningRandom Forest Algorithm in Machine LearningK-Nearest Neighbor(KNN) AlgorithmSupport Vector Machine (SVM) AlgorithmNaive Bayes ClassifiersUnsupervised LearningWhat is Unsupervised LearningK means Clustering – IntroductionHierarchical Clustering in Machine LearningDBSCAN Clustering in ML - Density based clusteringApriori AlgorithmFrequent Pattern Growth AlgorithmECLAT Algorithm - MLPrincipal Component Analysis (PCA)Model Evaluation and TuningEvaluation Metrics in Machine LearningRegularization in Machine LearningCross Validation in Machine LearningHyperparameter TuningUnderfitting and Overfitting in MLBias and Variance in Machine LearningAdvanced TechniquesReinforcement LearningSemi-Supervised Learning in MLSelf-Supervised Learning (SSL)Ensemble LearningMachine Learning PracticeMachine Learning Interview Questions and Answers100+ Machine Learning Projects with Source CodeML & Data ScienceExplore \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTransformers in Machine Learning\\n\\n\\n\\nLast Updated : \\n10 Dec, 2025\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nComments\\n\\n\\n\\n\\n\\n\\n\\nImprove\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSuggest changes\\n\\n\\n\\n\\n\\n\\n16 Likes\\n\\n\\n\\nLike\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReport\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTransformer is a neural network architecture used for performing machine learning tasks particularly in natural language processing (NLP) and computer vision. In 2017 Vaswani et al. published a paper \" Attention is All You Need\" in which the transformers architecture was introduced. The article explores the architecture, workings and applications of transformers. Need For Transformers Model in Machine Learning Transformer architecture uses an attention mechanism to process an entire sentence at once instead of reading words one by one. This is useful because older models work step by step and it helps overcome the challenges seen in models like RNNs and LSTMs. Traditional models like RNNs (Recurrent Neural Networks) suffer from the vanishing gradient problem which leads to long-term memory loss. RNNs process text sequentially meaning they analyze words one at a time. For example: In the sentence: \"XYZ went to France in 2019 when there were no cases of COVID and there he met the president of that country\" the word \"that country\" refers to \"France\". However RNN would struggle to link \"that country\" to \"France\" since it processes each word in sequence leading to losing context over long sentences. This limitation prevents RNNs from understanding the full meaning of the sentence.While adding more memory cells in LSTMs (Long Short-Term Memory networks) helped address the vanishing gradient issue they still process words one by one. This sequential processing means LSTMs can\\'t analyze an entire sentence at once.For example: The word \"point\" has different meanings in these two sentences:\"The needle has a sharp point.\" (Point = Tip)\"It is not polite to point at people.\" (Point = Gesture)Traditional models struggle with this context dependence, whereas Transformer model through its self-attention mechanism processes the entire sentence in parallel addressing these issues and making it significantly more effective at understanding context.Core Concepts of TransformersArchitecture of Transformer1. Self Attention Mechanism The self attention mechanism allows transformers to determine which words in a sentence are most relevant to each other. This is done using a scaled dot-product attention approach:Each word in a sequence is mapped to three vectors:Query (Q)Key (K)Value (V)Attention scores are computed as: \\\\text{Attention}(Q, K, V) = \\\\text{softmax} \\\\left( \\\\frac{QK^T}{\\\\sqrt{d_k}} \\\\right) VThese scores determine how much attention each word should pay to others.2. Multi-Head AttentionInstead of one attention mechanism, transformers use multiple attention heads running in parallel. Each head captures different relationships or patterns in the data, enriching the model’s understanding.3. Positional Encoding Unlike RNNs, transformers lack an inherent understanding of word order since they process data in parallel. To solve this problem Positional Encodings are added to token embeddings providing information about the position of each token within a sequence.4. Position-wise Feed-Forward NetworksThe Feed-Forward Networks consist of two linear transformations with a ReLU activation. It is applied independently to each position in the sequence.Mathematically:\\\\text{FFN}(x) = \\\\max(0, xW_1 + b_1)W_2 + b_2This transformation helps refine the encoded representation at each position.5. EmbeddingsTransformers cannot work with raw words as they need numbers. So, each input token (word or subword) is converted into a vector, called an embedding.Both encoder input tokens and decoder input tokens are converted into embeddings.These embeddings are trainable, meaning the model learns the best numeric representation for each token.The same weight matrix is shared for Encoder embeddings, Decoder embeddings and the final linear layer before softmaxThe embeddings are scaled by model to keep values stable before adding positional encoding.Embeddings turn words into meaningful numeric vectors that the transformer can process.6. Encoder-Decoder Architecture The encoder-decoder structure is key to transformer models. The encoder processes the input sequence into a vector, while the decoder converts this vector back into a sequence. Each encoder and decoder layer includes self-attention and feed-forward layers.For example, a French sentence \"Je suis étudiant\" is translated into \"I am a student\" in English.Transformers apply attention in three different places:1. Encoder Self-AttentionQ, K, V all come from the encoder’s previous layer.Every word can attend to every other word in the input.This helps the encoder understand full context (long-range meaning).2. Decoder Self-Attention (Masked)Q, K, V all come from the decoder’s previous layer.Future tokens are masked (blocked), so each position only sees previous tokens.This keeps decoding auto-regressive i.e the model predicts one word at a time.3. Encoder–Decoder AttentionQueries come from the decoder.Keys and Values come from the encoder output.This lets the decoder look at important parts of the input sentence while generating output.Together, these three attention types allow the transformer to read the entire input at once and then generate outputs step-by-step with full context.7. Softmax Layer for Output PredictionAfter the decoder processes the sequence, it must predict the next token.The decoder output is passed through a linear layer (whose weights are shared with embeddings).Then the softmax function converts these scores into probabilities.The token with the highest probability becomes the predicted next word.Intuition with ExampleFor instance in the sentence \"The cat didn\\'t chase the mouse, because it was not hungry\" the word \\'it\\' refers to \\'cat\\'. The self-attention mechanism helps the model correctly associate \\'it\\' with \\'cat\\' ensuring an accurate understanding of sentence structure.Applications Some of the applications of transformers are: NLP Tasks: Transformers are used for machine translation, text summarization, named entity recognition and sentiment analysis.Speech Recognition: They process audio signals to convert speech into transcribed text.Computer Vision: Transformers are applied to image classification, object detection and image generation.Recommendation Systems: They provide personalized recommendations based on user preferences.Text and Music Generation: Transformers are used for generating text like articles and composing music. \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCreate Quiz\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTransformers in Machine Learning\\n\\n\\nVisit Course\\n\\n\\n\\n\\n\\n\\n\\nTransformers in Machine Learning\\n\\n\\n\\nTransformers Explained | Natural Language Processing (NLP)\\n\\n \\n\\n\\n\\n\\n\\r\\n        Comment\\r\\n    \\n\\n\\n\\n\\n\\nE\\n\\n\\n\\n\\n\\nerror_502\\n\\n\\n\\n\\n\\n Follow\\n\\n\\n\\n\\n\\n\\n16\\n\\n\\nImprove\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\nE\\n\\n\\n\\n\\n \\n\\nerror_502 \\n\\n\\n\\n\\n\\n Follow \\n\\n\\n\\n\\n\\n\\n\\n\\n16\\n\\n\\n\\nImprove\\n\\n\\n\\n\\n\\n\\nArticle Tags : \\n\\n\\nMachine Learning\\n\\n\\nAI-ML-DS\\n\\n\\nDeep-Learning\\n \\n\\n\\n\\n\\n\\n\\n\\n\\nLike\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCorporate & Communications Address:\\n\\r\\n                      A-143, 7th Floor, Sovereign Corporate Tower, Sector- 136, Noida, Uttar Pradesh (201305)                    \\n\\n\\n\\n\\n\\nRegistered Address:\\r\\n                        K 061, Tower K, Gulshan Vivante Apartment, Sector 137, Noida, Gautam Buddh Nagar, Uttar Pradesh, 201305                      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCompanyAbout UsLegalPrivacy PolicyCareersContact UsCorporate SolutionCampus Training ProgramExplorePOTDJob-A-ThonConnectBlogsNation Skill UpTutorialsProgramming LanguagesDSAWeb TechnologyAI, ML & Data ScienceDevOpsCS Core SubjectsGATESchool SubjectsSoftware and ToolsCoursesML and Data ScienceDSA and PlacementsWeb DevelopmentData ScienceProgramming LanguagesDevOps & CloudGATETrending TechnologiesOffline CentersNoidaBengaluruPuneHyderabadPatnaPreparation CornerInterview CornerAptitudePuzzlesGfG 160System Design \\n\\n\\n\\n\\n\\n\\n@GeeksforGeeks, Sanchhaya Education Private Limited, All rights reserved\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nImprovement\\n\\n\\n\\n\\n\\n\\n\\nSuggest changes\\n\\n\\n\\n\\n\\nSuggest Changes\\nHelp us improve. Share your suggestions to enhance the article. Contribute your expertise and make a difference in the GeeksforGeeks portal.\\n\\n\\n\\n\\n\\n\\n\\nCreate Improvement\\nEnhance the article with your expertise. Contribute to the GeeksforGeeks community and help create better learning resources for all.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSuggest Changes\\n\\n\\n\\n\\n\\n\\nmin 4 words, max Words Limit:1000\\n\\n\\n\\n\\nThank You!\\nYour suggestions are valuable to us.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat kind of Experience do you want to share?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nInterview Experiences\\n\\n\\n\\n\\n\\n\\n\\nAdmission Experiences\\n\\n\\n\\n\\n\\n\\n\\nCareer Journeys\\n\\n\\n\\n\\n\\n\\n\\nWork Experiences\\n\\n\\n\\n\\n\\n\\n\\nCampus Experiences\\n\\n\\n\\n\\n\\n\\n\\nCompetitive Exam Experiences\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing ot loading the data sources\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader,TextLoader\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/\",)\n",
    ")\n",
    "#loader=TextLoader(\"c:\\DATA SCIENCE\\Machine_Learning\\Datasets\\Salary_dataset.csv\")\n",
    "# Choose the loader based on your file type: \n",
    "# loader = CSVLoader(sample_path) # For CSV \n",
    "# loader = TextLoader(\"path/to/file.txt\") \n",
    "# loader = PyPDFLoader(\"path/to/file.pdf\") \n",
    "# loader = UnstructuredWordDocumentLoader(\"path/to/file.docx\")\n",
    "docs = loader.load()\n",
    "\n",
    "assert len(docs) == 1\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f9c09ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchModuleError",
     "evalue": "Can't load plugin: sqlalchemy.dialects:http",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNoSuchModuleError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutilities\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SQLDatabase\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Replace with your XAMPP credentials\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m engine = \u001b[43mcreate_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttp://localhost/phpmyadmin/index.php?route=/sql&pos=0&db=new&table=titanic\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Wrap into LangChain SQLDatabase\u001b[39;00m\n\u001b[32m     10\u001b[39m db = SQLDatabase(engine)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:2\u001b[39m, in \u001b[36mcreate_engine\u001b[39m\u001b[34m(url, **kwargs)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prasa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sqlalchemy\\util\\deprecations.py:281\u001b[39m, in \u001b[36mdeprecated_params.<locals>.decorate.<locals>.warned\u001b[39m\u001b[34m(fn, *args, **kwargs)\u001b[39m\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[32m    275\u001b[39m         _warn_with_version(\n\u001b[32m    276\u001b[39m             messages[m],\n\u001b[32m    277\u001b[39m             versions[m],\n\u001b[32m    278\u001b[39m             version_warnings[m],\n\u001b[32m    279\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    280\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prasa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sqlalchemy\\engine\\create.py:568\u001b[39m, in \u001b[36mcreate_engine\u001b[39m\u001b[34m(url, **kwargs)\u001b[39m\n\u001b[32m    564\u001b[39m u = _url.make_url(url)\n\u001b[32m    566\u001b[39m u, plugins, kwargs = u._instantiate_plugins(kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m568\u001b[39m entrypoint = \u001b[43mu\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_entrypoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    569\u001b[39m _is_async = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33m_is_async\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    570\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_async:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prasa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sqlalchemy\\engine\\url.py:772\u001b[39m, in \u001b[36mURL._get_entrypoint\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    770\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    771\u001b[39m     name = \u001b[38;5;28mself\u001b[39m.drivername.replace(\u001b[33m\"\u001b[39m\u001b[33m+\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m772\u001b[39m \u001b[38;5;28mcls\u001b[39m = \u001b[43mregistry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    773\u001b[39m \u001b[38;5;66;03m# check for legacy dialects that\u001b[39;00m\n\u001b[32m    774\u001b[39m \u001b[38;5;66;03m# would return a module with 'dialect' as the\u001b[39;00m\n\u001b[32m    775\u001b[39m \u001b[38;5;66;03m# actual class\u001b[39;00m\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    777\u001b[39m     \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdialect\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    778\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mcls\u001b[39m.dialect, \u001b[38;5;28mtype\u001b[39m)\n\u001b[32m    779\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m.dialect, Dialect)\n\u001b[32m    780\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prasa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:453\u001b[39m, in \u001b[36mPluginLoader.load\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    450\u001b[39m         \u001b[38;5;28mself\u001b[39m.impls[name] = impl.load\n\u001b[32m    451\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m impl.load()\n\u001b[32m--> \u001b[39m\u001b[32m453\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc.NoSuchModuleError(\n\u001b[32m    454\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mCan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt load plugin: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % (\u001b[38;5;28mself\u001b[39m.group, name)\n\u001b[32m    455\u001b[39m )\n",
      "\u001b[31mNoSuchModuleError\u001b[39m: Can't load plugin: sqlalchemy.dialects:http"
     ]
    }
   ],
   "source": [
    "#sql files\n",
    "from sqlalchemy import create_engine\n",
    "from langchain_community.document_loaders import SQLDatabaseLoader\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "\n",
    "# Replace with your XAMPP credentials\n",
    "engine = create_engine(\"http://localhost/phpmyadmin/index.php?route=/sql&pos=0&db=new&table=titanic\")\n",
    "\n",
    "# Wrap into LangChain SQLDatabase\n",
    "db = SQLDatabase(engine)\n",
    "\n",
    "loader = SQLDatabaseLoader(db, query=\"SELECT * FROM customers;\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(documents[0].page_content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81dc03ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 16 sub-documents.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US', 'start_index': 48}, page_content=\"Transformers in Machine Learning - GeeksforGeeks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCoursesDSA / PlacementsGATE PrepML & Data ScienceDevelopmentCloud / DevOpsProgramming LanguagesAll CoursesTutorialsPythonJavaDSAML & Data ScienceInterview CornerProgramming LanguagesWeb DevelopmentGATECS SubjectsDevOpsSchool LearningSoftware and ToolsPracticePractice Coding ProblemsNation Skillup- Ending Soon!Problem of the DayAI Agent Workshop (with Certification)Study Abroad ChampionshipJobsApply Now!Post JobsJobs UpdatesApply for Campus Mantri\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNotifications\\n\\nMark all as read\\n\\n\\n\\n\\n\\nAll\\n\\n\\n\\n\\n\\n \\n\\nView All\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNotifications\\n\\n\\n\\nMark all as read\\n\\n\\n\\n\\n\\n\\nAll\\n\\n\\nUnread\\n\\n\\nRead\\n\\n\\n\\n\\n\\r\\n                        You're all caught up!!\"),\n",
       " Document(metadata={'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US', 'start_index': 689}, page_content=\"All\\n\\n\\n\\n\\n\\n \\n\\nView All\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNotifications\\n\\n\\n\\nMark all as read\\n\\n\\n\\n\\n\\n\\nAll\\n\\n\\nUnread\\n\\n\\nRead\\n\\n\\n\\n\\n\\r\\n                        You're all caught up!!\\r\\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPython for Machine LearningMachine Learning with RMachine Learning AlgorithmsEDAMath for Machine LearningMachine Learning Interview QuestionsML ProjectsDeep LearningNLPComputer visionData ScienceArtificial Intelligence \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign In\\n\\n\\n▲\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOpen In App\"),\n",
       " Document(metadata={'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US', 'start_index': 1167}, page_content='Technical Scripter ExploreMachine Learning BasicsIntroduction to Machine LearningTypes of Machine LearningWhat is Machine Learning Pipeline?Applications of Machine LearningPython for Machine LearningMachine Learning with Python TutorialNumPy Tutorial - Python LibraryPandas TutorialData Preprocessing in PythonEDA - Exploratory Data Analysis in PythonFeature EngineeringWhat is Feature Engineering?Introduction to Dimensionality ReductionFeature Selection Techniques in Machine LearningSupervised LearningSupervised Machine LearningLinear Regression in Machine learningLogistic Regression in Machine LearningDecision Tree in Machine LearningRandom Forest Algorithm in Machine LearningK-Nearest Neighbor(KNN) AlgorithmSupport Vector Machine (SVM) AlgorithmNaive Bayes ClassifiersUnsupervised LearningWhat is Unsupervised LearningK means Clustering – IntroductionHierarchical Clustering in Machine LearningDBSCAN Clustering in ML - Density based clusteringApriori AlgorithmFrequent Pattern Growth'),\n",
       " Document(metadata={'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US', 'start_index': 1971}, page_content='is Unsupervised LearningK means Clustering – IntroductionHierarchical Clustering in Machine LearningDBSCAN Clustering in ML - Density based clusteringApriori AlgorithmFrequent Pattern Growth AlgorithmECLAT Algorithm - MLPrincipal Component Analysis (PCA)Model Evaluation and TuningEvaluation Metrics in Machine LearningRegularization in Machine LearningCross Validation in Machine LearningHyperparameter TuningUnderfitting and Overfitting in MLBias and Variance in Machine LearningAdvanced TechniquesReinforcement LearningSemi-Supervised Learning in MLSelf-Supervised Learning (SSL)Ensemble LearningMachine Learning PracticeMachine Learning Interview Questions and Answers100+ Machine Learning Projects with Source CodeML & Data ScienceExplore'),\n",
       " Document(metadata={'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US', 'start_index': 2732}, page_content='Transformers in Machine Learning\\n\\n\\n\\nLast Updated : \\n10 Dec, 2025\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nComments\\n\\n\\n\\n\\n\\n\\n\\nImprove\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSuggest changes\\n\\n\\n\\n\\n\\n\\n16 Likes\\n\\n\\n\\nLike\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReport'),\n",
       " Document(metadata={'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US', 'start_index': 2926}, page_content='Transformer is a neural network architecture used for performing machine learning tasks particularly in natural language processing (NLP) and computer vision. In 2017 Vaswani et al. published a paper \" Attention is All You Need\" in which the transformers architecture was introduced. The article explores the architecture, workings and applications of transformers. Need For Transformers Model in Machine Learning Transformer architecture uses an attention mechanism to process an entire sentence at once instead of reading words one by one. This is useful because older models work step by step and it helps overcome the challenges seen in models like RNNs and LSTMs. Traditional models like RNNs (Recurrent Neural Networks) suffer from the vanishing gradient problem which leads to long-term memory loss. RNNs process text sequentially meaning they analyze words one at a time. For example: In the sentence: \"XYZ went to France in 2019 when there were no cases of COVID and there he met the'),\n",
       " Document(metadata={'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US', 'start_index': 3720}, page_content='memory loss. RNNs process text sequentially meaning they analyze words one at a time. For example: In the sentence: \"XYZ went to France in 2019 when there were no cases of COVID and there he met the president of that country\" the word \"that country\" refers to \"France\". However RNN would struggle to link \"that country\" to \"France\" since it processes each word in sequence leading to losing context over long sentences. This limitation prevents RNNs from understanding the full meaning of the sentence.While adding more memory cells in LSTMs (Long Short-Term Memory networks) helped address the vanishing gradient issue they still process words one by one. This sequential processing means LSTMs can\\'t analyze an entire sentence at once.For example: The word \"point\" has different meanings in these two sentences:\"The needle has a sharp point.\" (Point = Tip)\"It is not polite to point at people.\" (Point = Gesture)Traditional models struggle with this context dependence, whereas Transformer model'),\n",
       " Document(metadata={'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US', 'start_index': 4519}, page_content='two sentences:\"The needle has a sharp point.\" (Point = Tip)\"It is not polite to point at people.\" (Point = Gesture)Traditional models struggle with this context dependence, whereas Transformer model through its self-attention mechanism processes the entire sentence in parallel addressing these issues and making it significantly more effective at understanding context.Core Concepts of TransformersArchitecture of Transformer1. Self Attention Mechanism The self attention mechanism allows transformers to determine which words in a sentence are most relevant to each other. This is done using a scaled dot-product attention approach:Each word in a sequence is mapped to three vectors:Query (Q)Key (K)Value (V)Attention scores are computed as: \\\\text{Attention}(Q, K, V) = \\\\text{softmax} \\\\left( \\\\frac{QK^T}{\\\\sqrt{d_k}} \\\\right) VThese scores determine how much attention each word should pay to others.2. Multi-Head AttentionInstead of one attention mechanism, transformers use multiple attention'),\n",
       " Document(metadata={'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US', 'start_index': 5337}, page_content='\\\\right) VThese scores determine how much attention each word should pay to others.2. Multi-Head AttentionInstead of one attention mechanism, transformers use multiple attention heads running in parallel. Each head captures different relationships or patterns in the data, enriching the model’s understanding.3. Positional Encoding Unlike RNNs, transformers lack an inherent understanding of word order since they process data in parallel. To solve this problem Positional Encodings are added to token embeddings providing information about the position of each token within a sequence.4. Position-wise Feed-Forward NetworksThe Feed-Forward Networks consist of two linear transformations with a ReLU activation. It is applied independently to each position in the sequence.Mathematically:\\\\text{FFN}(x) = \\\\max(0, xW_1 + b_1)W_2 + b_2This transformation helps refine the encoded representation at each position.5. EmbeddingsTransformers cannot work with raw words as they need numbers. So, each input'),\n",
       " Document(metadata={'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US', 'start_index': 6138}, page_content='= \\\\max(0, xW_1 + b_1)W_2 + b_2This transformation helps refine the encoded representation at each position.5. EmbeddingsTransformers cannot work with raw words as they need numbers. So, each input token (word or subword) is converted into a vector, called an embedding.Both encoder input tokens and decoder input tokens are converted into embeddings.These embeddings are trainable, meaning the model learns the best numeric representation for each token.The same weight matrix is shared for Encoder embeddings, Decoder embeddings and the final linear layer before softmaxThe embeddings are scaled by model to keep values stable before adding positional encoding.Embeddings turn words into meaningful numeric vectors that the transformer can process.6. Encoder-Decoder Architecture The encoder-decoder structure is key to transformer models. The encoder processes the input sequence into a vector, while the decoder converts this vector back into a sequence. Each encoder and decoder layer includes'),\n",
       " Document(metadata={'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US', 'start_index': 6939}, page_content='structure is key to transformer models. The encoder processes the input sequence into a vector, while the decoder converts this vector back into a sequence. Each encoder and decoder layer includes self-attention and feed-forward layers.For example, a French sentence \"Je suis étudiant\" is translated into \"I am a student\" in English.Transformers apply attention in three different places:1. Encoder Self-AttentionQ, K, V all come from the encoder’s previous layer.Every word can attend to every other word in the input.This helps the encoder understand full context (long-range meaning).2. Decoder Self-Attention (Masked)Q, K, V all come from the decoder’s previous layer.Future tokens are masked (blocked), so each position only sees previous tokens.This keeps decoding auto-regressive i.e the model predicts one word at a time.3. Encoder–Decoder AttentionQueries come from the decoder.Keys and Values come from the encoder output.This lets the decoder look at important parts of the input sentence'),\n",
       " Document(metadata={'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US', 'start_index': 7740}, page_content='predicts one word at a time.3. Encoder–Decoder AttentionQueries come from the decoder.Keys and Values come from the encoder output.This lets the decoder look at important parts of the input sentence while generating output.Together, these three attention types allow the transformer to read the entire input at once and then generate outputs step-by-step with full context.7. Softmax Layer for Output PredictionAfter the decoder processes the sequence, it must predict the next token.The decoder output is passed through a linear layer (whose weights are shared with embeddings).Then the softmax function converts these scores into probabilities.The token with the highest probability becomes the predicted next word.Intuition with ExampleFor instance in the sentence \"The cat didn\\'t chase the mouse, because it was not hungry\" the word \\'it\\' refers to \\'cat\\'. The self-attention mechanism helps the model correctly associate \\'it\\' with \\'cat\\' ensuring an accurate understanding of sentence'),\n",
       " Document(metadata={'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US', 'start_index': 8530}, page_content='the mouse, because it was not hungry\" the word \\'it\\' refers to \\'cat\\'. The self-attention mechanism helps the model correctly associate \\'it\\' with \\'cat\\' ensuring an accurate understanding of sentence structure.Applications Some of the applications of transformers are: NLP Tasks: Transformers are used for machine translation, text summarization, named entity recognition and sentiment analysis.Speech Recognition: They process audio signals to convert speech into transcribed text.Computer Vision: Transformers are applied to image classification, object detection and image generation.Recommendation Systems: They provide personalized recommendations based on user preferences.Text and Music Generation: Transformers are used for generating text like articles and composing music.'),\n",
       " Document(metadata={'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US', 'start_index': 9321}, page_content='Create Quiz\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTransformers in Machine Learning\\n\\n\\nVisit Course\\n\\n\\n\\n\\n\\n\\n\\nTransformers in Machine Learning\\n\\n\\n\\nTransformers Explained | Natural Language Processing (NLP)\\n\\n \\n\\n\\n\\n\\n\\r\\n        Comment\\r\\n    \\n\\n\\n\\n\\n\\nE\\n\\n\\n\\n\\n\\nerror_502\\n\\n\\n\\n\\n\\n Follow\\n\\n\\n\\n\\n\\n\\n16\\n\\n\\nImprove\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\nE\\n\\n\\n\\n\\n \\n\\nerror_502 \\n\\n\\n\\n\\n\\n Follow \\n\\n\\n\\n\\n\\n\\n\\n\\n16\\n\\n\\n\\nImprove\\n\\n\\n\\n\\n\\n\\nArticle Tags : \\n\\n\\nMachine Learning\\n\\n\\nAI-ML-DS\\n\\n\\nDeep-Learning\\n \\n\\n\\n\\n\\n\\n\\n\\n\\nLike\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCorporate & Communications Address:\\n\\r\\n                      A-143, 7th Floor, Sovereign Corporate Tower, Sector- 136, Noida, Uttar Pradesh (201305)                    \\n\\n\\n\\n\\n\\nRegistered Address:\\r\\n                        K 061, Tower K, Gulshan Vivante Apartment, Sector 137, Noida, Gautam Buddh Nagar, Uttar Pradesh, 201305'),\n",
       " Document(metadata={'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US', 'start_index': 10037}, page_content='Registered Address:\\r\\n                        K 061, Tower K, Gulshan Vivante Apartment, Sector 137, Noida, Gautam Buddh Nagar, Uttar Pradesh, 201305                      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCompanyAbout UsLegalPrivacy PolicyCareersContact UsCorporate SolutionCampus Training ProgramExplorePOTDJob-A-ThonConnectBlogsNation Skill UpTutorialsProgramming LanguagesDSAWeb TechnologyAI, ML & Data ScienceDevOpsCS Core SubjectsGATESchool SubjectsSoftware and ToolsCoursesML and Data ScienceDSA and PlacementsWeb DevelopmentData ScienceProgramming LanguagesDevOps & CloudGATETrending TechnologiesOffline CentersNoidaBengaluruPuneHyderabadPatnaPreparation CornerInterview CornerAptitudePuzzlesGfG 160System Design \\n\\n\\n\\n\\n\\n\\n@GeeksforGeeks, Sanchhaya Education Private Limited, All rights reserved\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nImprovement\\n\\n\\n\\n\\n\\n\\n\\nSuggest changes'),\n",
       " Document(metadata={'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US', 'start_index': 10759}, page_content='@GeeksforGeeks, Sanchhaya Education Private Limited, All rights reserved\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nImprovement\\n\\n\\n\\n\\n\\n\\n\\nSuggest changes\\n\\n\\n\\n\\n\\nSuggest Changes\\nHelp us improve. Share your suggestions to enhance the article. Contribute your expertise and make a difference in the GeeksforGeeks portal.\\n\\n\\n\\n\\n\\n\\n\\nCreate Improvement\\nEnhance the article with your expertise. Contribute to the GeeksforGeeks community and help create better learning resources for all.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSuggest Changes\\n\\n\\n\\n\\n\\n\\nmin 4 words, max Words Limit:1000\\n\\n\\n\\n\\nThank You!\\nYour suggestions are valuable to us.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat kind of Experience do you want to share?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nInterview Experiences\\n\\n\\n\\n\\n\\n\\n\\nAdmission Experiences\\n\\n\\n\\n\\n\\n\\n\\nCareer Journeys\\n\\n\\n\\n\\n\\n\\n\\nWork Experiences\\n\\n\\n\\n\\n\\n\\n\\nCampus Experiences\\n\\n\\n\\n\\n\\n\\n\\nCompetitive Exam Experiences')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#splliting into chuncks\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")\n",
    "all_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d10086e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['7af22624-c25b-4c41-948b-17d3961a3de7', 'd07ffad1-6862-4762-98ee-db3faf9d3807', '1f5f02cf-f819-4980-953d-cd09752cb7d0']\n"
     ]
    }
   ],
   "source": [
    "#storing the documents\n",
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "839a6d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a88733a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x000001C0C5F16D50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "\n",
    "tools = [retrieve_context]\n",
    "# If desired, specify custom instructions\n",
    "prompt = (\n",
    "    \"You have access to a tool that retrieves context from a blog post. \"\n",
    "    \"Use the tool to help answer user queries.\"\n",
    ")\n",
    "agent = create_agent(model, tools, system_prompt=prompt)\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90323340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what is the overall view of the website\n",
      "what is transformer \n",
      "what is the use of transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_context (3de91b90-ff29-4348-8096-2a5ef084c664)\n",
      " Call ID: 3de91b90-ff29-4348-8096-2a5ef084c664\n",
      "  Args:\n",
      "    query: overall view of the website\n",
      "  retrieve_context (b5fad0ec-ad2f-4c7e-9618-14a5bceff1c1)\n",
      " Call ID: b5fad0ec-ad2f-4c7e-9618-14a5bceff1c1\n",
      "  Args:\n",
      "    query: what is transformer\n",
      "  retrieve_context (078d6c40-4a1e-461c-b312-d7f8cabd5fde)\n",
      " Call ID: 078d6c40-4a1e-461c-b312-d7f8cabd5fde\n",
      "  Args:\n",
      "    query: what is the use of transformer\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Source: {'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US', 'start_index': 2926}\n",
      "Content: Transformer is a neural network architecture used for performing machine learning tasks particularly in natural language processing (NLP) and computer vision. In 2017 Vaswani et al. published a paper \" Attention is All You Need\" in which the transformers architecture was introduced. The article explores the architecture, workings and applications of transformers. Need For Transformers Model in Machine Learning Transformer architecture uses an attention mechanism to process an entire sentence at once instead of reading words one by one. This is useful because older models work step by step and it helps overcome the challenges seen in models like RNNs and LSTMs. Traditional models like RNNs (Recurrent Neural Networks) suffer from the vanishing gradient problem which leads to long-term memory loss. RNNs process text sequentially meaning they analyze words one at a time. For example: In the sentence: \"XYZ went to France in 2019 when there were no cases of COVID and there he met the\n",
      "\n",
      "Source: {'source': 'https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/', 'title': 'Transformers in Machine Learning - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US', 'start_index': 8530}\n",
      "Content: the mouse, because it was not hungry\" the word 'it' refers to 'cat'. The self-attention mechanism helps the model correctly associate 'it' with 'cat' ensuring an accurate understanding of sentence structure.Applications Some of the applications of transformers are: NLP Tasks: Transformers are used for machine translation, text summarization, named entity recognition and sentiment analysis.Speech Recognition: They process audio signals to convert speech into transcribed text.Computer Vision: Transformers are applied to image classification, object detection and image generation.Recommendation Systems: They provide personalized recommendations based on user preferences.Text and Music Generation: Transformers are used for generating text like articles and composing music.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The website, GeeksforGeeks, is a comprehensive educational platform covering various domains such as computer science, programming, school education, upskilling, and more.\n",
      "\n",
      "A Transformer is a neural network architecture introduced in 2017, primarily used for machine learning tasks in Natural Language Processing (NLP) and computer vision. It utilizes an attention mechanism to process an entire sequence (like a sentence) at once, overcoming limitations of older sequential models like RNNs and LSTMs that suffer from issues like the vanishing gradient problem.\n",
      "\n",
      "The Transformer's attention mechanism allows it to understand the context of words in a sentence more effectively, for example, correctly associating pronouns with their referents. Its applications are diverse and include machine translation, text summarization, speech recognition, image classification, and even text and music generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "    \"what is the overall view of the website\\n\"\n",
    "    \"what is transformer \\n\"\n",
    "    \"what is the use of transformer\"\n",
    ")\n",
    "\n",
    "for event in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
