{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510ea3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain langchain-community langchain-openai sentence-transformers faiss-cpu pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696a5458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import CSVLoader, TextLoader, PyPDFLoader, UnstructuredWordDocumentLoader\n",
    "\n",
    "# Choose the loader based on your file type:\n",
    "loader = CSVLoader(r\"c:\\DATA SCIENCE\\Machine_Learning\\Datasets\\bank.csv\")  # For CSV\n",
    "# loader = TextLoader(\"path/to/file.txt\")\n",
    "# loader = PyPDFLoader(\"path/to/file.pdf\")\n",
    "# loader = UnstructuredWordDocumentLoader(\"path/to/file.docx\")\n",
    "\n",
    "docs = loader.load()\n",
    "print(f\"Loaded {len(docs)} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda809ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain-text-splitters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba2f30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess.py\n",
    "import pandas as pd\n",
    "\n",
    "SENSITIVE_COLS = [\"name\",\"phone\",\"email\",\"address\",\"account_number\",\"ssn\",\"pan\",\"ifsc\"]\n",
    "HIGH_RISK_COLS = [\"password_hint\",\"security_question\",\"raw_notes\"]\n",
    "\n",
    "def load_and_sanitize(csv_path: str, role: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Drop high-risk columns entirely\n",
    "    for c in HIGH_RISK_COLS:\n",
    "        if c in df.columns: df.drop(columns=[c], inplace=True)\n",
    "    # Redact PII by default\n",
    "    for c in SENSITIVE_COLS:\n",
    "        if c in df.columns:\n",
    "            if role in [\"support\",\"analyst\",\"exec\",\"guest\"]:  # all roles redact raw PII\n",
    "                df[c] = \"[REDACTED]\"\n",
    "    # Role-specific reductions\n",
    "    if role == \"guest\":\n",
    "        keep = [c for c in df.columns if c not in SENSITIVE_COLS]  # minimal columns\n",
    "        df = df[keep]\n",
    "    elif role == \"analyst\":\n",
    "        # Aggregate view example: keep numeric for stats\n",
    "        pass  # Keep as needed, avoid raw identifiers\n",
    "    return df\n",
    "\n",
    "def to_documents(df: pd.DataFrame, chunk_cols=None):\n",
    "    # Convert each row to a text document for indexing; include only allowed columns\n",
    "    if chunk_cols is None:\n",
    "        chunk_cols = df.columns.tolist()\n",
    "    docs = []\n",
    "    for _, row in df.iterrows():\n",
    "        text = \"\\n\".join(f\"{c}: {row[c]}\" for c in chunk_cols)\n",
    "        docs.append(text)\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8727a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a Secure RAG assistant for a bank dataset. You must:\n",
    "- Enforce role-based access. Only answer allowed intents for the user's role.\n",
    "- Never reveal personally identifiable information (PII) or credentials.\n",
    "- Prefer aggregates, anonymized insights, and generalized guidance.\n",
    "- If a request is sensitive or disallowed, refuse with a brief policy-based explanation.\n",
    "- Verify outputs: if any token resembles PII or sensitive data, replace with [REDACTED] or refuse.\n",
    "\n",
    "When unsure, err on the side of privacy. Provide citations to retrieved chunks only if they are non-sensitive.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30339751",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a data safety–first financial assistant. You MUST enforce strict privacy controls.\n",
    "Never reveal the following sensitive details verbatim, even if explicitly asked:\n",
    "- Full account numbers\n",
    "- Phone numbers\n",
    "- Email addresses\n",
    "- Exact balances\n",
    "- Exact salaries\n",
    "- Exact credit scores\n",
    "- Full identity details (addresses, PAN/Aadhaar, DOB)\n",
    "\n",
    "Safety rules:\n",
    "1) If a user asks for any restricted field, POLITELY REFUSE and explain the restriction.\n",
    "2) Offer safe alternatives: high-level summaries, aggregates, trends, anonymized stats, or masked values.\n",
    "3) Masking format:\n",
    "   - Account numbers: show only last 4 digits (e.g., ****-****-****-3456)\n",
    "   - Phone numbers: mask middle digits (e.g., 98****3210)\n",
    "   - Email: mask local part (e.g., p*****t@example.com)\n",
    "   - Balances/salaries/credit scores: provide ranges or percentile buckets (e.g., balance is between 250k–315k)\n",
    "4) When answering allowed analytics (spending trends, category totals, monthly aggregates), ensure NO leakage of restricted fields.\n",
    "5) If a query mixes allowed and restricted parts, refuse the restricted parts but still provide safe analytics for the rest.\n",
    "6) If the retriever returns snippets containing sensitive fields, treat them as confidential and follow the masking and refusal rules.\n",
    "7) Never provide raw source text that includes sensitive data. Summarize safely.\n",
    "\n",
    "Respond concisely and ensure every output complies with these rules.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee834a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "INTENT_PROMPT = \"\"\"\n",
    "Classify the user query into one of:\n",
    "- general_info, product_faq, troubleshooting, aggregate_stats, trend_analysis, anonymized_insights,\n",
    "- identity_lookup, account_specific, personal_contact, credentials, ssn_pan_lookup, account_number_reveal.\n",
    "\n",
    "Return JSON: {\"intent\": \"...\", \"sensitive\": true/false, \"reason\": \"...\"}.\n",
    "Be conservative: if unsure, mark sensitive=true.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dd58ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5566d481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filters.py\n",
    "import re\n",
    "import policy\n",
    "from policy import SECURITY_POLICY\n",
    "\n",
    "def scrub_sensitive(text: str) -> str:\n",
    "    out = text\n",
    "    for pat in SECURITY_POLICY[\"sensitive_patterns\"]:\n",
    "        out = re.sub(pat, \"[REDACTED]\", out)\n",
    "    return out\n",
    "\n",
    "def is_disallowed_intent(role: str, intent: str) -> bool:\n",
    "    role_pol = SECURITY_POLICY[\"roles\"].get(role, {})\n",
    "    return intent in role_pol.get(\"deny_intents\", [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91733f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.py\n",
    "import os, json\n",
    "import pandas as pd\n",
    "from preprocess import load_and_sanitize, to_documents\n",
    "from filters import scrub_sensitive, is_disallowed_intent\n",
    "from policy import SECURITY_POLICY, SECURITY_POLICY\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings  # or AzureOpenAIEmbeddings, or local (e.g., sentence-transformers via local runtime)\n",
    "from langchain.schema import Document\n",
    "from langchain.chat_models import ChatOpenAI   # or AzureChatOpenAI, or Ollama via ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "ROLE = os.environ.get(\"USER_ROLE\", \"analyst\")  # guest/support/analyst/exec\n",
    "\n",
    "# 1) Load and sanitize\n",
    "csv_path = \"c:\\DATA SCIENCE\\Machine_Learning\\Datasets\\bank.csv\"  # Provide path accessible to your runtime\n",
    "df = load_and_sanitize(csv_path, role=ROLE)\n",
    "docs_text = to_documents(df)\n",
    "\n",
    "# 2) Split and index\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=100)\n",
    "chunks = []\n",
    "for t in docs_text:\n",
    "    for c in splitter.split_text(t):\n",
    "        chunks.append(Document(page_content=c, metadata={\"role_view\": ROLE, \"source\": \"bank\"}))\n",
    "\n",
    "# 3) Embeddings & Vector store\n",
    "# Option A: OpenAI\n",
    "emb = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "vs = FAISS.from_documents(chunks, emb)\n",
    "\n",
    "# 4) Models\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "intent_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a conservative security classifier.\"),\n",
    "    (\"human\", \"{query}\\n\" + INTENT_PROMPT)\n",
    "])\n",
    "\n",
    "guarded_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM_PROMPT),\n",
    "    (\"system\", \"User role: {role}\\nIntent: {intent}\\nSensitive: {sensitive}\\nReason: {reason}\"),\n",
    "    (\"system\", \"Retrieved Context (sanitized):\\n{context}\"),\n",
    "    (\"human\", \"{query}\\nAnswer with strict compliance to policy.\")\n",
    "])\n",
    "\n",
    "# 5) Query function with policy enforcement\n",
    "def answer_query(query: str, k: int = 3):\n",
    "    # Classify intent\n",
    "    intent_chain = intent_prompt | llm\n",
    "    intent_raw = intent_chain.invoke({\"query\": query}).content\n",
    "    try:\n",
    "        meta = json.loads(intent_raw)\n",
    "    except:\n",
    "        meta = {\"intent\": \"general_info\", \"sensitive\": True, \"reason\": \"Parse fail; conservative default.\"}\n",
    "\n",
    "    if is_disallowed_intent(ROLE, meta[\"intent\"]) or meta.get(\"sensitive\", True):\n",
    "        return f\"Request denied due to privacy policy. Intent '{meta['intent']}' is not permitted for role '{ROLE}'.\"\n",
    "\n",
    "    # Retrieve minimal context\n",
    "    retrieved = vs.similarity_search(query, k=k)\n",
    "    sanitized_context = \"\\n---\\n\".join(scrub_sensitive(doc.page_content) for doc in retrieved)\n",
    "\n",
    "    # Compose guarded response\n",
    "    chain = guarded_prompt | llm\n",
    "    resp = chain.invoke({\n",
    "        \"role\": ROLE,\n",
    "        \"intent\": meta[\"intent\"],\n",
    "        \"sensitive\": meta[\"sensitive\"],\n",
    "        \"reason\": meta[\"reason\"],\n",
    "        \"context\": sanitized_context,\n",
    "        \"query\": query\n",
    "    }).content\n",
    "\n",
    "    # Final scrub just in case\n",
    "    resp = scrub_sensitive(resp)\n",
    "    return resp\n",
    "\n",
    "# Optional: simple audit logger\n",
    "def audit_log(query, role, decision, intent, reason):\n",
    "    print(json.dumps({\n",
    "        \"event\": \"audit\",\n",
    "        \"role\": role,\n",
    "        \"decision\": decision,\n",
    "        \"intent\": intent,\n",
    "        \"reason\": reason\n",
    "    }))\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    q = \"Show average balance by product segment without any names.\"\n",
    "    ans = answer_query(q)\n",
    "    print(ans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e903b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7bd9ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2222f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
